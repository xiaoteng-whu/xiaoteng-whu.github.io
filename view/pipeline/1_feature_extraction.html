<section>
    <div class="container">
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h1 class="text-center">Natural Feature Extraction</h1>
                <p>
                    The objective of this step is to extract distinctive groups of pixels that are, to some extent, invariant to changing camera viewpoints during image acquisition.
                    Hence, a feature in the scene should have similar feature descriptions in all images.
                </p>
                <p>
                    The most well-know feature detection method is the SIFT (Scale-invariant feature transform) algorithm.
                    The initial goal of SIFT is to extract discriminative patches in a first image that can be compared to discriminative patches of a second image irrespective of rotation, translation, and scale.
                    As a relevant detail only exists at a certain scale, the extracted patches are centered at stable points of interest.
                    The key idea is that, to some extent, one can use the SIFT invariance to deal with the image transformations occurring when the viewpoints are changing during image acquisition.
                </p>
                <p>
                    From the representation of one image at different scales, which is technically done by computing a pyramid of downscaled images.
                    SIFT computes scale-space maxima of the Laplacian representation, which is a specific image energy-based representation of the image, using so-called differences of Gaussians.
                    These maxima correspond to points of interest.
                    It then samples for each one of these maxima a square image patch whose origin is the maximum and x-direction is the dominant gradient at the origin.
                    For each keypoint, a description of these patches is associated.
                </p>
                <p>
                    The description, which is typically stored in 128 bits, consists of a statistics of gradients computed in regions around the keypoint.
                    The region size is determined by the keypoint scale and the orientation is determined by the dominant axis.
                </p>
                <p>
                    As the number of extracted features may vary a lot due to the variability of textures complexity (from one image to another or in different parts of the image), a post-filtering step is used to control the number of extracted features to reasonable limits (for instance between one and ten thousands per image).
                    We use a grid filtering to ensure a good repartition in the image.
                </p>
                <div class="references pmd-card pmd-card-default pmd-z-depth up_margin">
                    <div class="pmd-card-title">
                        <h2 class="pmd-card-title-text">References</h2>
                    </div>
                    <div class="pmd-card-body">
                        <div class="table-responsive">
                            <table class="table">
                                <tbody>
                                    <tr>
                                        <td data-title="code" id="Lowe2004">[Lowe2004]</td>
                                        <td data-title="name"><a target="_blank" href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">Distinctive image features from scale-invariant keypoints, David G. Lowe, 2004</a></td>
                                    </tr>
                                    <tr>
                                        <td data-title="code" id="Otero2014">[Otero2014]</td>
                                        <td data-title="name"><a target="_blank" href="http://www.ipol.im/pub/art/2014/82/">Anatomy of the SIFT Method, Ives Rey Otero, Mauricio Delbracio, 2014</a></td>
                                    </tr>
                                    <tr>
                                        <td data-title="code" id="Yu2011">[Yu2011]</td>
                                        <td data-title="name"><a target="_blank" href="http://www.ipol.im/pub/art/2011/my-asift/">ASIFT: An Algorithm for Fully Affine Invariant Comparison, Guoshen Yu, Jean-Michel Morel, 2011</a></td>
                                    </tr>
                                    <tr>
                                        <td data-title="code" id="Alcantarilla2013">[Alcantarilla2013]</td>
                                        <td data-title="name"><a target="_blank" href="http://www.bmva.org/bmvc/2013/Papers/paper0013/paper0013.pdf">AKAZE Fast explicit diffusion for accelerated features in nonlinear scale spaces, P.F. Alcantarilla, J. Nuevo, A. Bartoli, 2013</a></td>
                                    </tr>
                                </tbody>
                                <tbody>
                                    <tr>
                                        <td data-title="code" id="Li2015">[Li2015]</td>
                                        <td data-title="name"><a target="_blank" href="https://www.researchgate.net/profile/Yali_Li3/publication/273841042_A_survey_of_recent_advances_in_visual_feature_detection/links/5707d38408ae2eb9421bda3e.pdf">A survey of recent advances in visual feature detection, Yali Li, Shengjin Wang, Qi Tian, Xiaoqing Ding, 2015</a></td>
                                    </tr>
                                </tbody>
                                <tbody>
                                    <tr>
                                        <td data-title="code" id="VLFEAT2008">[VLFEAT2008]</td>
                                        <td data-title="name">
                                            <a target="_blank" href="http://www.vlfeat.org/">VLFeat: An Open and Portable Library of Computer Vision Algorithms A. Vedaldi and B. Fulkerson, 2008</a><br>
                                            <a target="_blank" href="http://www.vlfeat.org/overview/sift.html">VLFeat SIFT detailed presentation</a>
                                        </td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>